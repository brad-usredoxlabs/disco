#!/usr/bin/env node

import path from 'path'
import { fileURLToPath } from 'url'
import { promises as fs } from 'fs'
import { parseFrontMatter } from '../src/records/frontMatter.js'
import {
  extractRecordData,
  mergeMetadataAndFormData,
  composeRecordFrontMatter
} from '../src/records/jsonLdFrontmatter.js'
import { buildZodSchema } from '../src/records/zodBuilder.js'
import { loadSchemaBundle, readYaml, projectRoot } from './lib/loadSchemaBundle.mjs'
import { flattenFrontMatter, normalizeBaseIri } from './lib/jsonld.js'

const __dirname = path.dirname(fileURLToPath(import.meta.url))
const EXPORT_DIR = path.join(projectRoot, 'dist', 'ro-crate')
const INDEX_SOURCE_DIR = path.join(projectRoot, 'index')

async function main() {
  const bundleName = process.argv[2] || 'computable-lab'
  const bundle = await loadSchemaBundle(bundleName)
  const naming = bundle.naming
  if (!naming) {
    throw new Error(`Missing naming config for bundle "${bundleName}".`)
  }
  const baseIri = normalizeBaseIri(bundle.jsonLdConfig?.baseIri || '')

  const validators = buildValidators(bundle.recordSchemas)
  const records = []

  for (const [recordType, config] of Object.entries(naming)) {
    const baseDir = config?.baseDir
    if (!baseDir) continue
    const dirPath = path.join(projectRoot, baseDir)
    const files = await collectMarkdownFiles(dirPath)
    for (const filePath of files) {
      const relativePath = path.relative(projectRoot, filePath)
      const record = await loadRecord(filePath, recordType, bundle, validators[recordType])
      if (record) {
        records.push({ ...record, relativePath })
      }
    }
  }

  if (!records.length) {
    throw new Error('No records available to export. Ensure build-index succeeds first.')
  }

  await writeRoCrate(records, baseIri)
  console.log(`RO-Crate exported to ${EXPORT_DIR}`)
}

async function loadRecord(filePath, defaultType, bundle, validator) {
  const raw = await fs.readFile(filePath, 'utf8')
  if (!raw.trim().startsWith('---')) return null
  const { data, body } = parseFrontMatter(raw)
  if (!data) return null
  const frontMatter = data
  const recordType =
    frontMatter.metadata?.recordType ||
    frontMatter.recordType ||
    frontMatter.type ||
    defaultType
  if (!recordType) return null

  const { metadata, formData } = extractRecordData(recordType, frontMatter, bundle)
  const schemaPayload = mergeMetadataAndFormData(metadata, formData)
  if (validator) {
    const validationInput = stripUnknownFields(schemaPayload, bundle.recordSchemas?.[recordType])
    const result = validator.safeParse(validationInput)
    if (!result.success) {
      console.warn(`[export-rocrate] Skipping ${filePath} due to validation errors:`)
      result.error.issues.forEach((issue) => {
        console.warn(`  - ${issue.path.join('.') || 'root'}: ${issue.message}`)
      })
      return null
    }
  }
  const normalizedFrontMatter = composeRecordFrontMatter(recordType, schemaPayload, formData, bundle)
  const jsonLdNode = flattenFrontMatter(normalizedFrontMatter)
  if (!jsonLdNode['@id']) {
    console.warn(`[export-rocrate] Skipping ${filePath} because @id is missing after normalization.`)
    return null
  }
  return {
    recordType,
    jsonLd: jsonLdNode,
    body
  }
}

async function writeRoCrate(records, baseIri) {
  await fs.rm(EXPORT_DIR, { recursive: true, force: true }).catch(() => {})
  await fs.mkdir(EXPORT_DIR, { recursive: true })
  const indexArtifacts = await copyIndexArtifacts()
  const dataset = buildDatasetEntity(records, indexArtifacts, baseIri)
  const graph = [
    dataset,
    ...records.map((record) => buildRecordEntity(record)),
    ...indexArtifacts.map((artifact) => buildIndexEntity(artifact))
  ]
  const roCrate = {
    '@context': 'https://w3id.org/ro/crate/1.1/context',
    '@graph': graph
  }
  await fs.writeFile(path.join(EXPORT_DIR, 'ro-crate-metadata.json'), JSON.stringify(roCrate, null, 2), 'utf8')
  await fs.writeFile(
    path.join(EXPORT_DIR, 'README.txt'),
    'RO-Crate generated by scripts/export-rocrate.mjs. Contents mirror the YAML frontmatter (JSON-LD) for each record.',
    'utf8'
  )
}

function buildDatasetEntity(records, indexArtifacts = [], baseIri = '') {
  const indexIds = (indexArtifacts || []).map((artifact) => artifact.id)
  const dataset = {
    '@id': baseIri || './',
    '@type': 'Dataset',
    name: baseIri ? `DIsCo Pages export (${baseIri})` : 'DIsCo Pages RO-Crate export',
    description: baseIri
      ? `Records exported from ${baseIri}`
      : 'Records exported from DIsCo Pages 2.0',
    hasPart: [
      ...records.map((record) => record.jsonLd['@id'] || record.jsonLd.id).filter(Boolean),
      ...indexIds
    ]
  }
  if (baseIri) {
    dataset.identifier = baseIri
    dataset.url = baseIri
  }
  return dataset
}

function buildRecordEntity({ recordType, jsonLd }) {
  const entity = {
    '@id': jsonLd['@id'] || jsonLd.id || `${recordType}:${jsonLd.shortSlug || ''}`,
    '@type': jsonLd['@type'] || [`ex:${recordType}`],
    ...jsonLd
  }
  const biologyEntities = jsonLd?.biology?.entities
  if (Array.isArray(biologyEntities) && biologyEntities.length) {
    entity.biologyEntities = biologyEntities
  }
  return entity
}

function buildIndexEntity(artifact) {
  return {
    '@id': artifact.id,
    '@type': 'File',
    name: artifact.name,
    encodingFormat: artifact.encodingFormat,
    contentSize: artifact.contentSize,
    path: artifact.path
  }
}

async function collectMarkdownFiles(dir) {
  const files = []
  async function walk(current) {
    let entries
    try {
      entries = await fs.readdir(current, { withFileTypes: true })
    } catch {
      return
    }
    for (const entry of entries) {
      const fullPath = path.join(current, entry.name)
      if (entry.isDirectory()) {
        await walk(fullPath)
      } else if (entry.isFile() && /\.(md|markdown|ya?ml)$/i.test(entry.name)) {
        files.push(fullPath)
      }
    }
  }
  await walk(dir)
  return files
}

function buildValidators(recordSchemas) {
  const validators = {}
  const context = { schemas: recordSchemas }
  for (const [recordType, schema] of Object.entries(recordSchemas || {})) {
    if (!schema) continue
    try {
      validators[recordType] = buildZodSchema(schema, context)
    } catch (err) {
      console.warn(`[export-rocrate] Unable to compile validator for ${recordType}: ${err.message}`)
    }
  }
  return validators
}

function stripUnknownFields(payload, schema) {
  if (!payload || typeof payload !== 'object') return payload
  const allowed = new Set(Object.keys(schema?.properties || {}))
  if (!allowed.size) return payload
  const next = {}
  Object.entries(payload).forEach(([key, value]) => {
    if (allowed.has(key)) {
      next[key] = value
    }
  })
  return next
}

async function copyIndexArtifacts() {
  let entries = []
  try {
    entries = await fs.readdir(INDEX_SOURCE_DIR, { withFileTypes: true })
  } catch {
    return []
  }
  const files = entries.filter((entry) => entry.isFile())
  if (!files.length) return []
  const targetDir = path.join(EXPORT_DIR, 'index')
  await fs.mkdir(targetDir, { recursive: true })
  const artifacts = []
  for (const entry of files) {
    const src = path.join(INDEX_SOURCE_DIR, entry.name)
    const dest = path.join(targetDir, entry.name)
    await fs.copyFile(src, dest)
    const stats = await fs.stat(src)
    artifacts.push({
      id: `index/${entry.name}`,
      name: entry.name,
      path: `index/${entry.name}`,
      encodingFormat: inferEncoding(entry.name),
      contentSize: stats.size
    })
  }
  return artifacts
}

function inferEncoding(filename) {
  if (filename.endsWith('.json')) return 'application/json'
  if (filename.endsWith('.mjs') || filename.endsWith('.js')) return 'text/javascript'
  return 'application/octet-stream'
}

main().catch((err) => {
  console.error('[export-rocrate] Failed:', err)
  process.exitCode = 1
})
